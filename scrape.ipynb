{
 "metadata": {
  "name": "",
  "signature": "sha256:6e8d073a62735ab6bbe610d6c66ecd4d6d89773157e863adf39e0dc7f2611e4d"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\"\"\"\n",
      "Scrape edgar for all the xbrl filings from one company, extract all of \n",
      "the facts accross all the filings and output a unified set of facts by \n",
      "instant and period.\n",
      "\"\"\"\n",
      "import os, re\n",
      "from itertools import groupby\n",
      "import requests, requests_cache\n",
      "from bs4 import BeautifulSoup"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Download all the filings from edgar\n",
      "ticker = 'AAPL'\n",
      "requests_cache.install_cache(\"/data/swan/%s\" % ticker)\n",
      "\n",
      "edgar_search_url = \"http://www.sec.gov/cgi-bin/browse-edgar?action=getcompany&type={0}&output=atom&CIK={1}&count=100&start=0\"\n",
      "entries = BeautifulSoup(requests.get(edgar_search_url.format(\"10\", ticker)).text).find_all('entry')\n",
      "page_urls = [entry.find('filing-href').text for entry in entries if entry.find('xbrl_href')]\n",
      "pages = [requests.get(url).text for url in page_urls]\n",
      "regex = re.compile(\"href=\\\"(.*xml)\\\"\")\n",
      "urls = [\"http://www.sec.gov{0}\".format(regex.findall(p)[0]) for p in pages]\n",
      "filings = [requests.get(url).text for url in urls]\n",
      "print \"Found\", len(filings), \"filings\""
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Found 21 filings\n"
       ]
      }
     ],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Find all the unique instant's and their associated contexts\n",
      "s = BeautifulSoup(filings[0])\n",
      "instants = [c for c in s(\"context\") if c(\"instant\")]\n",
      "unique_instants = [(k, [i.attrs['id'] for i in list(v)]) for k, v in groupby(instants, lambda x: x.period.instant.text)]\n",
      "\n",
      "# [((i[0], s.find_all(contextref=re.compile(c))) for c in i[1]) for i in unique_instants]\n",
      "\n",
      "instant_entities = [((i[0], [e for e in s.find_all(contextref=re.compile(c))]) for c in i[1]) for i in unique_instants]\n",
      "\n",
      "# instant_entities = [(i[0], [e for e in s.find_all(contextref=re.compile(str(c))) for c in i[1]]) for i in unique_instants]\n",
      "# instant_facts = [(i[0], list(set([(f.name, f.string) for f in i[1]]))) for i in instant_entities]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def extract_gaap_entities(instants):\n",
      "    for instant in instants:\n",
      "        for context in instant[1]:\n",
      "            for e in s.find_all(contextref=re.compile(context)):\n",
      "                if  \"xsi:nil\" not in e.attrs and e.name.startswith(\"us-gaap\"):\n",
      "                    yield((instant[0], e.name, float(e.contents[0])))\n",
      "\n",
      "gaap_instant_facts = extract_gaap_entities(unique_instants)\n",
      "\n",
      "print gaap_instant_facts.next()\n",
      "print gaap_instant_facts.next()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "(u'2013-09-28', u'us-gaap:availableforsalesecuritiescurrent', 0.0)\n",
        "(u'2013-09-28', u'us-gaap:availableforsalesecuritiesnoncurrent', 0.0)\n"
       ]
      }
     ],
     "prompt_number": 97
    }
   ],
   "metadata": {}
  }
 ]
}