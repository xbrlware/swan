{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Net vs. Goldman\n",
    "Can an LTSM Neural Net trained on fundamentals extracted from edgar XBRL from the S&P500 pick the same long and short list as Goldman's hedgefund meta list (appologies to my high school english teacher for the run on sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Sources\n",
    "Download price and fundamental data for the S&P 500 using [pystock-crawler](https://github.com/eliangcs/pystock-crawler):  \n",
    "\n",
    "`pystock-crawler reports ../tickers.csv -o ../reports.csv ../reports.log`  \n",
    "`pystock-crawler prices ../tickers.csv -o ../prices.csv -l ../prices.log`\n",
    "\n",
    "REMIND: Use pystock-crawler symbols to get symbols for training input...\n",
    "\n",
    "X = 4 quarters of fundamental data and whether stock was up or down from the prior quarter y = whether the stock was up or down x days after the period that X was comprised of"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 502 symbols and 10851 reports\n"
     ]
    }
   ],
   "source": [
    "import pandas\n",
    "prices = pandas.read_csv('prices.csv', parse_dates=['date'], index_col=1)\n",
    "reports = pandas.read_csv('reports.csv', parse_dates=['date'], index_col=1)\n",
    "reports = reports[reports.amend == False]\n",
    "symbols = pandas.read_csv('symbols.csv').sort('symbol').sort('symbol')\n",
    "print \"Loaded\", len(symbols), \"symbols and\", len(reports), \"reports\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def features(symbol, reports, prices, window_size=4, overlap=1, days_after=10):\n",
    "    \"\"\"\n",
    "    Return a 2d vector consisting of the data for the given stock split into multiple\n",
    "    possibly overlapping windows along with a y value showing the % change in the \n",
    "    stock price days_after the end of the window.\n",
    "    \"\"\"\n",
    "    num_windows = len(reports[reports.symbol == symbol]) - window_size + 1\n",
    "    # Create a set of sequences of reports returning X, y\n",
    "    r = reports[reports.symbol == symbol].sort(ascending=True)\n",
    "    p = prices[prices.symbol == symbol].sort(ascending=True)\n",
    "    print \"Found\", len(r), \"reports for\", symbol\n",
    "\n",
    "    # Add closing stock price to each report\n",
    "    r['close'] = r.index.map(lambda x: p.ix[p.index[p.index.searchsorted(x)]]['close'])\n",
    "\n",
    "    # Fixup annual 10-k numbers by subtracting the prior 3 quarters\n",
    "    # REMIND: Go back and verify the adjustments\n",
    "    for c in ['revenues', 'op_income', 'net_income',\n",
    "     'eps_basic', 'eps_diluted', 'dividend',\n",
    "     'cash_flow_op', 'cash_flow_inv', u'cash_flow_fin']:\n",
    "        r[c + '_adj'] = r[c] - r[c].shift(1) - r[c].shift(2) - r[c].shift(3)\n",
    "        r.ix[r.period_focus == 'FY', c] = r[r.period_focus == 'FY'][c + '_adj']\n",
    "        \n",
    "    # Delete all non-numeric columns\n",
    "    r = r.ix[:,5:-9]\n",
    "    \n",
    "    # Change any nan to -1\n",
    "    r.fillna(-1, inplace=True)\n",
    "    \n",
    "    # Divide into overlapping windows\n",
    "    X = [r[i:i + window_size] for i in range(len(r) - window_size + 1 - num_windows,len(r) - window_size + 1, overlap)]\n",
    "\n",
    "    # Calculate % rise in stock price n days after the last report in the window\n",
    "    y = [1 - p.ix[p.index[p.index.searchsorted(x.index[3]) + days_after]].close / x.ix[3].close for x in X]\n",
    "    \n",
    "    return [x.values for x in X], y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 22 reports for A\n",
      "Found 24 reports for AET\n",
      "Found 24 reports for APA\n",
      "Found 24 reports for CBS\n",
      "Found 25 reports for CNX\n",
      "Found 20 reports for DAL\n",
      "Found 25 reports for DTV\n",
      "Found 13 reports for ESRX\n",
      "Found 24 reports for FITB\n",
      "Found 23 reports for GME\n",
      "Found 24 reports for HES\n",
      "Found 23 reports for IP\n",
      "Found 19 reports for KMX\n",
      "Found 19 reports for LRCX\n",
      "Found 17 reports for MJN\n",
      "Found 20 reports for NWL\n",
      "Found 24 reports for PFE\n",
      "Found 19 reports for PVH\n",
      "Found 24 reports for RTN\n",
      "Found 24 reports for STJ\n",
      "Found 23 reports for TMK\n",
      "Found 19 reports for URBN\n",
      "Found 16 reports for WFM\n",
      "Generated features vectors for 23 stocks\n"
     ]
    }
   ],
   "source": [
    "data = {s: features(s, reports, prices) \n",
    "        for s in symbols['symbol'][0:500:20] if len(reports[reports.symbol == s]) >= 4}\n",
    "print \"Generated features vectors for\", len(data),\"stocks\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "assert(np.array_equal(data['A'][0][0][1], data['A'][0][1][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num train symbols: 18 num test symbols: 5\n",
      "test/train % 0.277777777778\n"
     ]
    }
   ],
   "source": [
    "# Split into train/test sets\n",
    "# Should use sklearn.cross_validation.StratifiedShuffleSplit to try and maintain industry sector % in each\n",
    "# Or bin by financial size http://www.gregreda.com/2013/10/26/using-pandas-on-the-movielens-dataset/\n",
    "# s = XY['symbol'].unique()\n",
    "from sklearn.cross_validation import train_test_split\n",
    "train_symbols, test_symbols = train_test_split(data.keys(), test_size = 0.2)\n",
    "print \"num train symbols:\", len(train_symbols), \"num test symbols:\", len(test_symbols)\n",
    "X_train = [data[s][0] for s in train_symbols]\n",
    "y_train = [data[s][1] for s in train_symbols]\n",
    "X_test = [data[s][0] for s in test_symbols]\n",
    "y_test = [data[s][1] for s in test_symbols]\n",
    "print \"test/train %\", 1.0 * len(X_test)/len(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def flatten(l):\n",
    "    return [item.astype(np.float32) for sublist in l for item in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy\n",
    "with open(\"train_test.npz\", \"wb\") as f:\n",
    "    numpy.savez(f, X_train=flatten(X_train), y_train=flatten(y_train), X_test=flatten(X_test), y_test=flatten(y_test))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
